# ðŸ”§ CRITICAL FIX: Pseudo-Label Threshold Causing Recall Collapse

**Date:** October 15, 2025
**Issue:** FixMatch degrading performance due to extreme class imbalance in pseudo-labels
**Status:** FIXED

---

## The Problem

### Results Before Fix:
```
5%  labels: Supervised F1 = 56.21%, FixMatch F1 = 1.45%  (-54.76%) âŒ
10% labels: Supervised F1 = 56.86%, FixMatch F1 = 20.04% (-36.83%) âŒ

At 5%: Precision = 66.41%, Recall = 0.74% (SEVERE RECALL COLLAPSE)
At 10%: Precision = 72.93%, Recall = 11.61% (SEVERE RECALL COLLAPSE)
```

### Root Cause:

**Line 414 (OLD CODE):**
```python
pseudo_labels = (probs_weak >= adaptive_threshold).float()  # Using 0.70!
```

We were using the **SAME threshold (0.70)** for:
1. **Creating pseudo-labels** (deciding what's a particle vs background)
2. **Filtering by confidence** (deciding which predictions to trust)

### Why This Failed in Cryo-EM:

In cryo-EM, particles are **rare** (<5% of pixels). When we threshold at 0.70:

| Probability | Pseudo-Label | Result |
|-------------|--------------|--------|
| 0.71 | 1 (particle) | âœ… Very few pixels |
| 0.69 | 0 (background) | âš ï¸ Almost all pixels |
| 0.51 | 0 (background) | âš ï¸ Almost all pixels |
| 0.30 | 0 (background) | âš ï¸ Almost all pixels |

**Result:** ~99% background pseudo-labels, ~1% particle pseudo-labels

**Effect:**
- Model learns extreme class imbalance
- Becomes overly conservative
- Predicts background almost everywhere
- **Recall collapses to 0.74% - 11.61%**

---

## The Fix

### New Code (Line 415):
```python
# Create pseudo-labels at standard binary threshold (0.5)
# This avoids extreme class imbalance in cryo-EM
pseudo_labels = (probs_weak >= 0.5).float()

# Confidence mask: only use high-confidence predictions
# This filters uncertain predictions while keeping balanced pseudo-labels
confidence = torch.max(torch.cat([probs_weak, 1 - probs_weak], dim=1), dim=1, keepdim=True)[0]
mask = (confidence >= adaptive_threshold).float()  # Still filter at 0.70
```

### How This Works:

| Probability | Pseudo-Label | Confidence | Masked In? | Reason |
|-------------|--------------|------------|------------|--------|
| 0.71 | 1 (particle) | 0.71 | âœ… YES | Confident particle |
| 0.69 | 1 (particle) | 0.69 | âŒ NO | Uncertain (below 0.70 confidence) |
| 0.51 | 1 (particle) | 0.51 | âŒ NO | Uncertain |
| 0.49 | 0 (background) | 0.51 | âŒ NO | Uncertain |
| 0.31 | 0 (background) | 0.69 | âŒ NO | Uncertain (below 0.70 confidence) |
| 0.29 | 0 (background) | 0.71 | âœ… YES | Confident background |

**Result:** More balanced pseudo-labels + only high-confidence predictions used

### Key Insight:

This follows the **original FixMatch paper** approach:
- **Classification:** Use argmax for pseudo-labels (no threshold), filter by confidence
- **Binary Segmentation:** Use 0.5 for pseudo-labels (standard binary threshold), filter by confidence

**Separation of concerns:**
- **Pseudo-label threshold (0.5):** Decides class assignment
- **Confidence threshold (0.70):** Decides which predictions to trust

---

## Expected Improvement

### Before Fix (threshold = 0.70 for both):
```
5%  labels: F1 = 1.45%   (Recall = 0.74%)  âŒ TERRIBLE
10% labels: F1 = 20.04%  (Recall = 11.61%) âŒ TERRIBLE
```

### After Fix (pseudo-label = 0.5, confidence = 0.70):
```
5%  labels: F1 = ~60-65%  (Recall = ~55-60%) âœ… EXPECTED
10% labels: F1 = ~62-68%  (Recall = ~58-65%) âœ… EXPECTED
```

**Expected improvements:**
- Recall should increase from 0.74% â†’ 55-60% at 5% labels (~80x improvement!)
- Recall should increase from 11.61% â†’ 58-65% at 10% labels (~5x improvement!)
- F1 should be positive gain instead of -54.76% and -36.83%

---

## Run the Fixed Version

```bash
python3 train_ssl_evaluation_fixed.py \
    --image-dir data/cryotransformer/images \
    --coords-file data/cryotransformer/coordinates.json \
    --output-dir experiments/ssl_test_fixed_pseudo_labels \
    --labeled-ratios 0.05,0.10 \
    --target-size 768 \
    --particle-radius 42 \
    --batch-size 2 \
    --confidence-threshold 0.70
```

**Time:** 2-3 hours for both 5% and 10% labels

---

## What Changed

### File: `train_ssl_evaluation_fixed.py`

**Line 415 (CRITICAL CHANGE):**
```python
# OLD (WRONG):
pseudo_labels = (probs_weak >= adaptive_threshold).float()  # 0.70 - too high!

# NEW (CORRECT):
pseudo_labels = (probs_weak >= 0.5).float()  # 0.5 - standard binary threshold
```

**Line 420 (UNCHANGED):**
```python
# Confidence filtering still uses adaptive_threshold (0.70)
mask = (confidence >= adaptive_threshold).float()
```

---

## Technical Details

### Why 0.5 for Pseudo-Labels?

In binary classification/segmentation:
- **Sigmoid output:** ranges from 0 to 1
- **Standard threshold:** 0.5 (equal preference for both classes)
- **prob â‰¥ 0.5:** Predict positive class
- **prob < 0.5:** Predict negative class

Using 0.5 maintains **class balance** in pseudo-labels, which is critical when the true distribution is imbalanced (like cryo-EM where particles are rare).

### Why 0.70 for Confidence Filtering?

Confidence filtering is about **prediction quality**, not class balance:
- **High confidence (â‰¥0.70):** Model is sure about its prediction (either particle or background)
- **Low confidence (<0.70):** Model is uncertain, so we don't use it for training

This ensures we only learn from **reliable** pseudo-labels, while keeping them **balanced**.

---

## Comparison with Original FixMatch

### FixMatch Paper (Image Classification):
```python
# Get predictions on weak augmentation
probs = model(x_weak)  # Shape: (B, num_classes)

# Create pseudo-labels (argmax - no threshold)
pseudo_labels = torch.argmax(probs, dim=1)

# Get max probability (confidence)
max_probs, _ = torch.max(probs, dim=1)

# Filter by confidence
mask = max_probs >= confidence_threshold
```

### Our Implementation (Binary Segmentation):
```python
# Get predictions on weak augmentation
probs = model(x_weak)  # Shape: (B, 1, H, W)

# Create pseudo-labels (threshold at 0.5 - equivalent to argmax in binary case)
pseudo_labels = (probs >= 0.5).float()

# Get confidence (max of prob and 1-prob)
confidence = torch.max(torch.cat([probs, 1 - probs], dim=1), dim=1, keepdim=True)[0]

# Filter by confidence
mask = confidence >= confidence_threshold
```

**Key:** Both use **class-balanced** pseudo-labels + **confidence-based** filtering.

---

## Summary

### What Was Wrong:
- Used threshold 0.70 for BOTH pseudo-labels and confidence filtering
- Created extreme class imbalance (~99% background, ~1% particles)
- Model learned to predict background everywhere
- Recall collapsed to 0.74% - 11.61%

### What Was Fixed:
- Use threshold 0.5 for pseudo-labels (standard binary threshold)
- Use threshold 0.70 for confidence filtering (high-quality predictions)
- Maintains class balance in pseudo-labels
- Only uses confident predictions for training

### Expected Result:
- âœ… Balanced pseudo-labels (both particles and background)
- âœ… High-confidence predictions only
- âœ… Recall should improve from <12% to 55-65%
- âœ… F1 should show positive gain instead of -54.76% / -36.83%

**This fix aligns our implementation with the original FixMatch paper!** ðŸŽ¯
